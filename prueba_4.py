#!/usr/bin/env python3
"""
prueba_4.py - Script de prueba para el scraper de Alkosto

PROP√ìSITO:
Este script prueba la nueva funcionalidad del AlkostoScraper que permite
cargar TODOS los productos (155 en total) haciendo clic autom√°ticamente 
en el bot√≥n "Mostrar m√°s productos" usando Selenium.

FUNCIONALIDAD PROBADA:
- Carga din√°mica de productos mediante clics en bot√≥n "Mostrar m√°s"
- Extracci√≥n de URLs de todos los productos disponibles
- Manejo de esperas y timeouts para carga AJAX
- Detecci√≥n autom√°tica cuando no hay m√°s productos

PROBLEMA RESUELTO:
Antes solo pod√≠amos obtener los primeros 25 productos porque Alkosto
usa carga din√°mica en lugar de paginaci√≥n por URL. Ahora el scraper
simula clics del usuario para cargar todos los productos.

EJECUCI√ìN:
python prueba_4.py

SALIDA ESPERADA:
- Log detallado del proceso de carga
- Lista de URLs de productos encontrados
- Archivo JSON con todos los productos para verificaci√≥n
"""

import json
import sys
import os
from datetime import datetime
from pathlib import Path

# Agregar el directorio src al path para importar m√≥dulos
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

try:
    from src.infraestructure.scrapers.alkosto_scraper import AlkostoScraper
    from loguru import logger
except ImportError as e:
    print(f"‚ùå Error importando m√≥dulos: {e}")
    print("üí° Aseg√∫rate de estar en la ra√≠z del proyecto y que src/ existe")
    sys.exit(1)

def main():
    """
    Funci√≥n principal que ejecuta la prueba del scraper de Alkosto.
    """
    print("="*60)
    print("üß™ PRUEBA 4: Scraper de Alkosto con carga din√°mica")
    print("="*60)
    print()
    print("üìã OBJETIVO:")
    print("   Probar que el scraper puede cargar TODOS los productos (155)")
    print("   usando clics autom√°ticos en 'Mostrar m√°s productos'")
    print()
    print("‚ö†Ô∏è  NOTA:")
    print("   Esta prueba abre un navegador Chrome y toma unos minutos")
    print("   Se recomienda ejecutar sin modo headless la primera vez")
    print()
    
    # Preguntar al usuario si quiere continuar
    response = input("¬øContinuar con la prueba? (s/n): ").lower().strip()
    if response not in ['s', 'si', 'y', 'yes']:
        print("‚ùå Prueba cancelada por el usuario")
        return
    
    print("\nüöÄ Iniciando prueba...")
    
    # Configurar logging m√°s detallado
    logger.remove()  # Remover configuraci√≥n por defecto
    logger.add(
        sys.stdout, 
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
        level="INFO"
    )
    
    # Crear instancia del scraper
    print("\nüì¶ Creando instancia del AlkostoScraper...")
    scraper = AlkostoScraper(delay_between_requests=1.0, max_retries=3)
    
    # Variables para medir el rendimiento
    start_time = datetime.now()
    
    try:
        print("\nüéØ Ejecutando get_product_urls() para obtener TODOS los productos...")
        print("   (Esto puede tomar 3-5 minutos dependiendo de la velocidad de internet)")
        
        # NOTA: Cambiar max_products para pruebas m√°s r√°pidas
        # Para prueba completa: max_products=155
        # Para prueba r√°pida: max_products=50
        product_urls = scraper.get_product_urls_with_load_more(max_products=155)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "="*60)
        print("üìä RESULTADOS DE LA PRUEBA")
        print("="*60)
        print(f"‚úÖ Productos encontrados: {len(product_urls)}")
        print(f"‚è±Ô∏è  Tiempo total: {duration}")
        print(f"‚ö° Velocidad: {len(product_urls)/duration.total_seconds():.2f} productos/segundo")
        
        if len(product_urls) >= 100:
            print("üéâ ¬°√âXITO! Se cargaron m√°s de 100 productos")
        elif len(product_urls) >= 50:
            print("‚úÖ PARCIAL: Se cargaron m√°s de 50 productos")
        else:
            print("‚ö†Ô∏è  LIMITADO: Se cargaron menos de 50 productos")
        
        # Mostrar algunas URLs de ejemplo
        if product_urls:
            print(f"\nüìã Primeras 5 URLs encontradas:")
            for i, url in enumerate(product_urls[:5], 1):
                print(f"   {i}. {url}")
            
            if len(product_urls) > 5:
                print(f"   ... y {len(product_urls) - 5} m√°s")
        
        # Guardar resultados en archivo JSON para verificaci√≥n
        output_file = f"prueba_4_resultados_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        results = {
            "timestamp": datetime.now().isoformat(),
            "total_productos": len(product_urls),
            "tiempo_ejecucion": str(duration),
            "productos_por_segundo": len(product_urls)/duration.total_seconds(),
            "urls": product_urls
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Resultados guardados en: {output_file}")
        
        # Prueba adicional: scraping de un producto
        if product_urls:
            print(f"\nüîç PRUEBA ADICIONAL: Scraping de un producto individual...")
            try:
                sample_product = scraper.scrape_product(product_urls[0])
                print(f"‚úÖ Producto scrapeado exitosamente:")
                print(f"   Nombre: {sample_product.nombre}")
                print(f"   Precio: ${sample_product.precio:,.0f}")
                print(f"   Marca: {sample_product.marca}")
                print(f"   Tama√±o: {sample_product.tamano_pulgadas}\"")
                print(f"   Resoluci√≥n: {sample_product.resolucion}")
            except Exception as e:
                print(f"‚ùå Error en scraping individual: {e}")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Prueba interrumpida por el usuario")
        return
        
    except Exception as e:
        print(f"\n‚ùå ERROR durante la prueba: {e}")
        logger.exception("Detalles del error:")
        return
    
    print("\nüèÅ Prueba completada!")
    print("\nüí° PR√ìXIMOS PASOS:")
    print("   1. Si la prueba fue exitosa, puedes usar el scraper en producci√≥n")
    print("   2. Ajusta max_products seg√∫n tus necesidades")
    print("   3. Considera agregar modo headless para ejecuci√≥n autom√°tica")

def verificar_dependencias():
    """
    Verifica que todas las dependencias est√©n instaladas.
    """
    # Mapeo de nombres de paquetes pip a nombres de m√≥dulos Python
    dependencias = {
        'selenium': 'selenium',
        'beautifulsoup4': 'bs4',  # beautifulsoup4 se importa como bs4
        'loguru': 'loguru',
        'webdriver-manager': 'webdriver_manager'
    }
    
    faltantes = []
    for pip_name, module_name in dependencias.items():
        try:
            __import__(module_name)
        except ImportError:
            faltantes.append(pip_name)
    
    if faltantes:
        print("‚ùå Dependencias faltantes:")
        for dep in faltantes:
            print(f"   - {dep}")
        print(f"\nInstala con: pip install {' '.join(faltantes)}")
        return False
    
    return True

if __name__ == "__main__":
    # Verificar dependencias antes de ejecutar
    if not verificar_dependencias():
        print("\nüí° Instala las dependencias y vuelve a ejecutar la prueba")
        sys.exit(1)
    
    # Ejecutar prueba principal
    main()