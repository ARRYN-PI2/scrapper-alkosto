"""
PRUEBA 3: Escalabilidad del Scraper de Alkosto
==============================================

OBJETIVO:
Probar quÃ© tan bien escala el scraper con mÃºltiples pÃ¡ginas y productos.
Evaluar rendimiento, estabilidad y capacidad de procesamiento masivo.

QUÃ‰ SE PRUEBA:
1. âœ… Scraping de mÃºltiples pÃ¡ginas (2-5 pÃ¡ginas)
2. âœ… Tiempo de procesamiento y eficiencia
3. âœ… Manejo de errores en productos individuales
4. âœ… Estabilidad del navegador con muchas peticiones
5. âœ… Diversidad de marcas y productos encontrados
6. âœ… DetecciÃ³n automÃ¡tica del final de pÃ¡ginas

RESULTADOS ESPERADOS:
- Encontrar 50-150 productos Ãºnicos
- Tiempo razonable por pÃ¡gina (1-2 minutos)
- Alta tasa de Ã©xito en extracciÃ³n de datos
- Diversidad de marcas, precios y caracterÃ­sticas
"""

import sys
import os
from loguru import logger
import time
from collections import Counter

# Agregar el directorio src al path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.infraestructure.scrapers.alkosto_scraper import AlkostoScraper
from src.domain.entities.televisor import Televisor


def test_escalabilidad():
    """Prueba de escalabilidad con mÃºltiples pÃ¡ginas."""
    
    logger.info("ğŸš€ Iniciando Prueba 3: Escalabilidad")
    logger.info("=" * 60)
    
    # ConfiguraciÃ³n de la prueba
    MAX_PAGES = 3  # Empezar con 3 pÃ¡ginas
    DELAY_BETWEEN_REQUESTS = 2.0  # Un poco mÃ¡s lento para ser respetuosos
    
    scraper = AlkostoScraper(delay_between_requests=DELAY_BETWEEN_REQUESTS)
    
    # Fase 1: Obtener URLs de mÃºltiples pÃ¡ginas
    logger.info(f"ğŸ“„ FASE 1: Obteniendo URLs de {MAX_PAGES} pÃ¡ginas")
    start_time = time.time()
    
    product_urls = scraper.get_product_urls(max_pages=MAX_PAGES)
    
    urls_time = time.time() - start_time
    logger.info(f"â±ï¸ Tiempo para obtener URLs: {urls_time:.1f} segundos")
    logger.info(f"ğŸ“Š URLs encontradas: {len(product_urls)}")
    logger.info(f"âš¡ Promedio: {len(product_urls)/urls_time:.1f} URLs/segundo")
    
    if len(product_urls) == 0:
        logger.error("âŒ No se encontraron URLs. Prueba fallida.")
        return
    
    # Mostrar muestra de URLs encontradas
    logger.info(f"\nğŸ“‹ Muestra de URLs encontradas:")
    for i, url in enumerate(product_urls[:10], 1):
        logger.info(f"  {i}. {url}")
    if len(product_urls) > 10:
        logger.info(f"  ... y {len(product_urls) - 10} URLs mÃ¡s")
    
    # Fase 2: Scrapear una muestra representativa de productos
    logger.info(f"\nğŸ“º FASE 2: Scrapeando muestra de productos")
    
    # Tomar una muestra para no saturar (mÃ¡ximo 20 productos para la prueba)
    sample_size = min(20, len(product_urls))
    sample_urls = product_urls[:sample_size]
    
    logger.info(f"ğŸ¯ Procesando muestra de {sample_size} productos...")
    
    televisores = []
    errores = []
    start_scraping = time.time()
    
    for i, url in enumerate(sample_urls, 1):
        try:
            logger.info(f"ğŸ“º Producto {i}/{sample_size}: Procesando...")
            
            product_start = time.time()
            televisor = scraper.scrape_product(url)
            product_time = time.time() - product_start
            
            televisores.append(televisor)
            
            # Log resumido para no saturar
            logger.info(f"  âœ… {televisor.marca} {televisor.tamano_pulgadas}\" - ${televisor.precio:,.0f} ({product_time:.1f}s)")
            
        except Exception as e:
            errores.append({'url': url, 'error': str(e)})
            logger.warning(f"  âŒ Error: {str(e)[:50]}...")
            continue
    
    scraping_time = time.time() - start_scraping
    
    # Fase 3: AnÃ¡lisis de resultados
    logger.info(f"\nğŸ“Š FASE 3: AnÃ¡lisis de resultados")
    logger.info("=" * 60)
    
    # EstadÃ­sticas bÃ¡sicas
    total_productos = len(televisores)
    tasa_exito = (total_productos / sample_size) * 100 if sample_size > 0 else 0
    
    logger.info(f"ğŸ“ˆ ESTADÃSTICAS GENERALES:")
    logger.info(f"  ğŸ“„ PÃ¡ginas procesadas: {MAX_PAGES}")
    logger.info(f"  ğŸ”— URLs encontradas: {len(product_urls)}")
    logger.info(f"  ğŸ“º Productos procesados: {total_productos}/{sample_size}")
    logger.info(f"  âœ… Tasa de Ã©xito: {tasa_exito:.1f}%")
    logger.info(f"  â±ï¸ Tiempo total scraping: {scraping_time:.1f} segundos")
    logger.info(f"  âš¡ Promedio por producto: {scraping_time/sample_size:.1f} segundos")
    
    if televisores:
        # AnÃ¡lisis de marcas
        marcas = Counter(tv.marca for tv in televisores)
        logger.info(f"\nğŸ·ï¸ MARCAS ENCONTRADAS:")
        for marca, cantidad in marcas.most_common():
            logger.info(f"  {marca}: {cantidad} productos")
        
        # AnÃ¡lisis de precios
        precios = [tv.precio for tv in televisores if tv.precio > 0]
        if precios:
            logger.info(f"\nğŸ’° ANÃLISIS DE PRECIOS:")
            logger.info(f"  Menor: ${min(precios):,.0f}")
            logger.info(f"  Mayor: ${max(precios):,.0f}")
            logger.info(f"  Promedio: ${sum(precios)/len(precios):,.0f}")
        
        # AnÃ¡lisis de tamaÃ±os
        tamanos = Counter(tv.tamano_pulgadas for tv in televisores if tv.tamano_pulgadas > 0)
        logger.info(f"\nğŸ“ TAMAÃ‘OS ENCONTRADOS:")
        for tamano, cantidad in sorted(tamanos.items()):
            logger.info(f"  {tamano}\": {cantidad} productos")
        
        # AnÃ¡lisis de resoluciones
        resoluciones = Counter(tv.resolucion for tv in televisores)
        logger.info(f"\nğŸ¯ RESOLUCIONES:")
        for resolucion, cantidad in resoluciones.most_common():
            logger.info(f"  {resolucion}: {cantidad} productos")
    
    # AnÃ¡lisis de errores
    if errores:
        logger.warning(f"\nâŒ ERRORES ENCONTRADOS ({len(errores)}):")
        for error in errores[:5]:  # Mostrar solo los primeros 5
            logger.warning(f"  - {error['error'][:80]}...")
    
    # ProyecciÃ³n de escalabilidad
    logger.info(f"\nğŸš€ PROYECCIÃ“N DE ESCALABILIDAD:")
    if len(product_urls) > 0 and scraping_time > 0:
        tiempo_por_producto = scraping_time / sample_size
        productos_totales_estimados = len(product_urls)
        tiempo_total_estimado = productos_totales_estimados * tiempo_por_producto / 60  # en minutos
        
        logger.info(f"  ğŸ“Š Productos totales disponibles: ~{productos_totales_estimados}")
        logger.info(f"  â±ï¸ Tiempo estimado para todos: ~{tiempo_total_estimado:.1f} minutos")
        logger.info(f"  ğŸ’¡ Productos por hora: ~{3600/tiempo_por_producto:.0f}")
    
    # Recomendaciones
    logger.info(f"\nğŸ’¡ RECOMENDACIONES:")
    if tasa_exito >= 90:
        logger.success("  âœ… Excelente estabilidad - Listo para producciÃ³n")
    elif tasa_exito >= 75:
        logger.info("  âš ï¸ Buena estabilidad - Algunos ajustes menores")
    else:
        logger.warning("  âŒ Revisar manejo de errores antes de producciÃ³n")
    
    tiempo_por_producto = scraping_time / sample_size if sample_size > 0 else 0
    if tiempo_por_producto < 2:
        logger.success("  âœ… Velocidad excelente")
    elif tiempo_por_producto < 5:
        logger.info("  âš¡ Velocidad aceptable")
    else:
        logger.warning("  â³ Considerar optimizaciones de velocidad")
    
    return {
        'total_urls': len(product_urls),
        'productos_procesados': total_productos,
        'tasa_exito': tasa_exito,
        'tiempo_scraping': scraping_time,
        'marcas_encontradas': len(marcas) if televisores else 0,
        'televisores': televisores
    }


def main():
    """FunciÃ³n principal"""
    # Configurar logging
    logger.remove()
    logger.add(
        sys.stdout,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
        level="INFO"
    )
    
    try:
        resultados = test_escalabilidad()
        
        logger.info(f"\nğŸ‰ PRUEBA 3 COMPLETADA")
        logger.info("=" * 60)
        
        if resultados['tasa_exito'] >= 75:
            logger.success("âœ… Scraper aprobÃ³ la prueba de escalabilidad")
            logger.info("ğŸ“‹ Listo para implementar guardado en JSON y MongoDB")
        else:
            logger.warning("âš ï¸ Scraper necesita ajustes antes de producciÃ³n")
        
        # Pregunta sobre continuar con scraping completo
        if resultados['total_urls'] > 20:
            respuesta = input(f"\nÂ¿Quieres probar scraping completo de todos los {resultados['total_urls']} productos? (puede tardar mucho) [y/N]: ")
            if respuesta.lower() in ['y', 'yes', 'sÃ­', 'si']:
                logger.info("ğŸš€ Iniciando scraping completo...")
                scraper = AlkostoScraper(delay_between_requests=1.5)
                todos_los_televisores = scraper.scrape_all_products(max_pages=3)
                logger.success(f"ğŸ‰ Scraping completo: {len(todos_los_televisores)} productos extraÃ­dos")
            else:
                logger.info("ğŸ“‹ Continuando con el siguiente paso del proyecto")
        
    except KeyboardInterrupt:
        logger.info("âŒ Prueba interrumpida por el usuario")
    except Exception as e:
        logger.error(f"âŒ Error inesperado: {e}")


if __name__ == "__main__":
    main()